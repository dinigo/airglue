enabled: true
schedule_interval: "0 2 * * *"
timezone: "Europe/London"
params: # Params to be loaded and available as {{ params }}
  default_dataset: airglue_example
  envs: # Environment variables to be loaded and available as {{ params.envs.<name> }}, undefined ones will be omitted
    - AIRGLUE_GCP_PROJECT_ID
  vars: # Airflow variables to be loaded and available as {{ params.vars.<name> }}, undefined ones will be omitted
    - example_bucket_name
tasks:
  - identifier: gcs_to_bq_example_us_states
    operator: airflow.contrib.operators.gcs_to_bq.GoogleCloudStorageToBigQueryOperator
    operator_factory: airglue.contrib.operator_factory.default.DefaultOperatorFactory
    arguments:
      bucket: "cloud-samples-data"
      source_objects:
        - "bigquery/us-states/us-states.csv"
      destination_project_dataset_table: "{{ params.default_dataset }}.gcs_to_bq_table_us_states_csv"
      write_disposition: "WRITE_TRUNCATE"
      source_format: "CSV"
      skip_leading_rows: 1
      schema_fields:
        - name: "name"
          type: "STRING"
          mode: "NULLABLE"
        - name: "post_abbr"
          type: "STRING"
          mode: "NULLABLE"
  - identifier: gcs_to_bq_example_us_states_avro
    operator: airflow.contrib.operators.gcs_to_bq.GoogleCloudStorageToBigQueryOperator
    operator_factory: airglue.contrib.operator_factory.default.DefaultOperatorFactory
    arguments:
      bucket: "cloud-samples-data"
      source_objects:
        - "bigquery/us-states/us-states.avro"
      destination_project_dataset_table: "{{ params.default_dataset }}.gcs_to_bq_table_us_states_avro"
      write_disposition: "WRITE_TRUNCATE"
      source_format: "AVRO"
  - identifier: done
    operator: airflow.operators.dummy_operator.DummyOperator
    operator_factory: airglue.contrib.operator_factory.default.DefaultOperatorFactory
    dependencies:
      - gcs_to_bq_example_us_states
      - gcs_to_bq_example_us_states_avro

